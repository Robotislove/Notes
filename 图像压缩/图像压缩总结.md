<<<<<<< HEAD
# 图像压缩总结
=======
# Generative Adversarial Networks for Extreme Learned Image Compression
>>>>>>> 0e3eb5a (2021.11)

## 主要文献
1. [ICCV 2019]Generative Adversarial Networks for Extreme Learned Image Compression
2. Generative Neural Network Based Image Compression

## 主要内容
传统的有损图像压缩技术(如 JPEG 和 WebP )并不是专门为要压缩的数据设计的，因此并没有达到图像的可能最佳压缩率。在已有深度学习模型的基础上，设计了一个基于GAN的图像压缩系统，在满足恢复质量的前提下，实现了极高的压缩率。模型结合了编码器、解码器/生成器和判别器，同时训练解码器/生成器和鉴别器，以此来实现压缩目标，与高质量的JPEG等标准方法相比，压缩率提高了数量级，同时能够生成更直观的重建，保持对原始图像的更高保真度。最后，使用PSNR和MS-SSIM标准度量方法对模型进行评估，并将其与JPEG、WebP和已有的深度学习压缩模型等作比较，结果证实基于GAN的压缩模型在压缩率更高的情况下，比已有的压缩方案结果更好。

网络的架构是基于论文 Perceptual Losses for Real-Time Style Transfer and Super-Resolution 中的附录中提供的描述完成的，项目中最初提到的多规格鉴别器的损失是基于论文 High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs 完成的。

生成器网络从低维潜在空间映射到高维图像空间。

对于下面的公式，我们将 latent space 潜空间称为z空间，将图像空间称为X空间。从X到z的映射并不是GAN网络的固有特性，但是这种映射正是压缩所必需的编码步骤。 因此，从z空间中随机初始化的向量通过随机梯度下降进行训练，其重建目标是在适当的损失函数下尽可能接近原始图像。

在生成器模型中，条件变量y实际上是作为一个额外的输入层（additional input layer），它与生成器的噪声输入p(z)组合形成了一个联合的隐层表达；在判别器模型中，y与真实数据x也是作为输入，并输入到一个判别函数当中。实际上就是将z和x分别于y进行concat，分别作为生成器和判别器的输入，再来进行训练。

CGAN在mnist数据集上进行了实验，对于生成器：使用数字的类别y作为标签，并进行了one-hot编码，噪声z来自均均匀分布；噪声z映射到200维的隐层，类别标签映射到1000维的隐层，然后进行拼接作为下一层的输入，激活函数使用ReLU；最后一层使用Sigmoid函数，生成的样本为784维。

## Feature matching Loss
论文地址：Improved Techniques for training GANS

GANs的loss是建立在discriminator的输出上的，即discriminator输出的交叉熵，这可能导致GANs训练不稳定，毕竟给予generator的信息太少了，而图像空间又太大了。为了让训练更稳定，作者提出了feature matching的方法。所谓的feature matching，即是要求generator产生的图像在经过discriminator时，提取的特征尽可能地接近（匹配）自然图像经过discriminator时提取的特征。设discriminator的某个中间层对于输入x的输出为f(x)，作者提出的feature matching，实际上是用下面的loss function替换以前的交叉熵loss： ￼ 这个loss比交叉熵loss更加直接，对generator的指导作用更大一些。

## [high-fidelity-generative-compression](https://github.com/Justin-Tan/high-fidelity-generative-compression)
This repository defines a model for learnable image compression capable of compressing images of arbitrary size and resolution based on the paper "High-Fidelity Generative Image Compression" (HIFIC) by Mentzer et. al..
There are three main components to this model, as described in the original paper:
1. An auto-encoding architecture defining a nonlinear transform to latent space. This is used in place of the linear transforms used by traditional image codecs.
2. A hierarchical (two-level in this case) entropy model over the quantized latent representation enabling lossless compression through standard entropy coding.
3. A generator-discriminator component that encourages the decoder/generator component to yield realistic reconstructions.

The model is then trained end-to-end by optimization of a modified rate-distortion Lagrangian. Loosely, the model defines an 'amortization' of the storage requirements for an generic input image by learning the weights used in the neural compression/decompression scheme. The method is further described in the original paper [0]. The model is capable of yielding perceptually similar reconstructions to the input that tend to be more visually pleasing than standard image codecs which operate at comparable or higher bitrates.

The model weights range between 1.5-2GB on disk, making transmission of the model itself impractical. The idea is that the same model is instantiated and made available to a sender and receiver. The sender encodes messages into the compressed format, which is transmitted via some channel to the receiver, who then decodes the compressed representation into a lossy reconstruction of the original data.

This repository also includes a partial port of the Tensorflow Compression library for general tools for neural image compression.
