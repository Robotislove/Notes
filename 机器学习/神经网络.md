# 神经网络

无论是线性回归还是逻辑回归都有这样一个缺点，即：当特征太多时，计算的负荷会非常大。

## 模型表示Model Representation

大脑中的神经网络：每一个神经元都可以被认为是一个处理单元/神经核（**processing unit**/**Nucleus**），它含有许多输入/树突（**input**/**Dendrite**），并且有一个输出/轴突（**output**/**Axon**）。神经网络是大量神经元相互链接并通过电脉冲来交流的一个网络。

![](assets/fbb4ffb48b64468c384647d45f7b86b5.png)

其中$x_1$, $x_2$, $x_3$是输入单元（**input units**），我们将原始数据输入给它们。$a_1$, $a_2$, $a_3$是中间单元，它们负责将数据进行处理，然后呈递到下一层。最后是输出单元，它负责计算${h_\theta}\left( x \right)$。

神经网络模型是许多逻辑单元按照不同层级组织起来的网络，每一层的输出变量都是下一层的输入变量。下图为一个3层的神经网络，第一层成为输入层（**Input Layer**），最后一层称为输出层（**Output Layer**），中间一层成为隐藏层（**Hidden Layers**）。我们为每一层都增加一个偏差单位（**bias unit**）：$x_0$, $a_0$

$a_{i}^{\left( j \right)}$ 代表第$j$ 层的第 $i$ 个激活单元。${{\Theta }^{\left( j \right)}}$代表从第 $j$ 层映射到第$ j+1$ 层时的权重的矩阵，例如${{\Theta }^{\left( 1 \right)}}$代表从第一层映射到第二层的权重的矩阵。其尺寸为：以第 $j+1$层的激活单元数量为行数，以第 $j$ 层的激活单元数加一为列数的矩阵（加一个偏置项）。例如：上图所示的神经网络中${{\Theta }^{\left( 1 \right)}}$的尺寸为 3*4。

对于上图所示的模型，激活单元和输出分别表达为：

$a_{1}^{(2)}=g(\Theta _{10}^{(1)}{{x}_{0}}+\Theta _{11}^{(1)}{{x}_{1}}+\Theta _{12}^{(1)}{{x}_{2}}+\Theta _{13}^{(1)}{{x}_{3}})$

$a_{2}^{(2)}=g(\Theta _{20}^{(1)}{{x}_{0}}+\Theta _{21}^{(1)}{{x}_{1}}+\Theta _{22}^{(1)}{{x}_{2}}+\Theta _{23}^{(1)}{{x}_{3}})$

$a_{3}^{(2)}=g(\Theta _{30}^{(1)}{{x}_{0}}+\Theta _{31}^{(1)}{{x}_{1}}+\Theta _{32}^{(1)}{{x}_{2}}+\Theta _{33}^{(1)}{{x}_{3}})$

${{h}_{\Theta }}(x)=g(\Theta _{10}^{(2)}a_{0}^{(2)}+\Theta _{11}^{(2)}a_{1}^{(2)}+\Theta _{12}^{(2)}a_{2}^{(2)}+\Theta _{13}^{(2)}a_{3}^{(2)})$

上面进行的讨论中只是将特征矩阵中的一行（一个训练实例）喂给了神经网络，我们需要将整个训练集都喂给我们的神经网络算法来学习模型。可以知道：每一个$a$都是由上一层所有的$x$和每一个$x$所对应$\theta$的决定的，把这样从左到右的算法称为前向传播算法forward propagation。

向量化表示

$x=\left[ \begin{matrix} x_0 \\ x_1 \\ x_2 \\ x_3 \end{matrix} \right]$ $z^{(2)} = \left[ \begin{matrix} z_1^{(2)} \\ z_2^{(2)} \\ z_3^{(2)} \end{matrix} \right]$

$z^{(2)} =\Theta^{(1)} x$
$a^{(2)} = g(z^{(2)})$

$a^{(2)} = \left[ \begin{matrix} a_1^{(2)} \\ a_2^{(2)} \\ a_3^{(2)} \end{matrix} \right] = g \left(\left[ \begin{matrix} \Theta_{10}^{(1)}&\Theta_{11}^{(1)}&\Theta_{12}^{(1)}&\Theta_{13}^{(1)} \\ \Theta_{20}^{(1)}&\Theta_{21}^{(1)}&\Theta_{22}^{(1)}&\Theta_{23}^{(1)} \\ \Theta_{30}^{(1)}&\Theta_{31}^{(1)}&\Theta_{32}^{(1)}&\Theta_{33}^{(1)} \\ \end{matrix} \right] \times\left[ \begin{matrix} x_0 \\ x_1 \\ x_2 \\ x_3 \end{matrix} \right] \right) = g \left(\left[ \begin{matrix} \Theta_{10}^{(1)}x_0&\Theta_{11}^{(1)}x_1&\Theta_{12}^{(1)}x_2&\Theta_{13}^{(1)}x_3 \\ \Theta_{20}^{(1)}x_0&\Theta_{21}^{(1)}x_1&\Theta_{22}^{(1)}x_2&\Theta_{23}^{(1)} x_3\\ \Theta_{30}^{(1)}x_0&\Theta_{31}^{(1)}x_1&\Theta_{32}^{(1)}x_2&\Theta_{33}^{(1)}x_3 \\ \end{matrix} \right]  \right)$

令 ${{z}^{\left( 2 \right)}}={{\Theta }^{\left( 1 \right)}}x$，则 ${{a}^{\left( 2 \right)}}=g({{z}^{\left( 2 \right)}})$ ，计算后添加 $a_{0}^{\left( 2 \right)}=1$。 计算输出的值为：

$h_\Theta(x) = a^{(3)} = g(z^{(3)} = g \left(\left[ \begin{matrix} \Theta_{10}^{(2)}&\Theta_{11}^{(2)}&\Theta_{12}^{(2)}&\Theta_{13}^{(2)} \end{matrix} \right] \times \left[ \begin{matrix} a_0^{(2)} \\ a_1^{(2)} \\ a_2^{(2)} \\ a_3^{(2)} \end{matrix} \right] \right) = g (\left[ \begin{matrix} \Theta_{10}^{(2)}a_0^{(2)}&\Theta_{11}^{(2)}a_1^{(2)}&\Theta_{12}^{(2)}a_2^{(2)}&\Theta_{13}^{(2)}a_3^{(2)} \end{matrix} \right] )$
