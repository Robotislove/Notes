# 2020年计算机视觉值得研究的领域
## [原文](https://www.zhihu.com/question/366016644/answer/971983556)

毫无疑问，3d方向，是非常值得研究的，包括深度估计，立体匹配，3d检测（包括单目，双目，lidar和rgbd，19年也终于出现了真正的点云卷积pointconv），3d分割，三维重建，3dlandmark，并且我个人认为如何减少3d标注，完全使用多视图几何做是一个很有意义，有前途，并且有挑战的方向。

## 视频方向
更新一下，补充一个非常重要的方向，视频方向，也就是考虑时间维度的cv，这包括运动目标检测，目标跟踪，运动语义分割。目标跟踪受相关滤波启发的一系列siamese工作已经非常漂亮了，剩下运动目标检测，运动语义分割，大体有几种思路：

1. conv+lstm（memory based），slow fast架构，还有两者的结合，另外还有基于光流的架构，在已知光流的情况下，通过前向warp或者后向warp，能在时间维度上前后转移featuremap，这是基本的出发点。个人其实挺喜欢光流的，因为如果不追求end2end的话，光流可以被用在很多地方（**当然，如果考虑时间的话，memory based方法产生的feature map也可以用在其他任何地方，只是不像光流那样可以从网络里面拆出来**），当然对于特别追求精度的地方，e2e会更好。memory based方面的工作我个人非常推崇google的looking fast and slow。

![20210824190857](https://raw.githubusercontent.com/lhondong/PicGo/main/img/20210824190857.png)
<center>memory结合slowfast，fast的参数一般很少。架构是通用的，修改head它能被用在其他任何task上</center>

![20210824191124](https://raw.githubusercontent.com/lhondong/PicGo/main/img/20210824191124.png)
<center>slowfast交错在一起（并且可以是异步的），能同时提高检测分割等其他各类任务的精度和速度</center>

2. **当然光流也可以e2e，光流完全可以作为conv+lstm或者slowfast的旁支输出，然后作用在featuremap上，但是一般深度学习光流的计算量都比较大，需要在一个比较大的区域内做匹配。并且如果联合训练的话，flow本身的自监督算法不一定是使用，比如unflow之类的算法。**

3. **memory based和flow based方法的结合点会非常有趣，或者说是否可以通过memory去估计flow，因为memory可以和slowfast架构结合，从而减小计算量。**

4. **3d卷积，随着tcn崛起成为新的序列建模方法，时间卷积和空间卷积可以合成成为3d卷积，另外slowfast架构里面，fast可以看成dilation rate更大的时间卷积，这方面的代表工作有c3d，i3d，tsn等，另外不得不提19年的Temporal Shift Module，它利用了时间卷积基本都是前向这个特点，用移位极大的减小了计算量**。从数字图像开始，本人就是卷积的忠实粉丝，我个人热爱一切全卷积架构。

![20210824191510](https://raw.githubusercontent.com/lhondong/PicGo/main/img/20210824191510.png)
<center>光流可以把feature map在时间维度上前向后向warp。这决定了flow的另一个好处，它能找到两帧计算结果之间的对应关系。flow的缺点是计算量可能会稍大</center>

## 3D方向
3-D部分具体说来包括
1. 单目深度估计如何提高计算性能，如何提高自监督的鲁棒性，前者有fastdepth，在tx2上已经能达到140fps的预测性能，后者包括monodepth2 ，struct2depth和geonet等一系列工作，使用多视图几何和运动估计来进行自监督，loss一般都是重投影误差的推广。struct2depth使用了一个预训练的实例分割模型，这样做的好处是能够单独建模每个物体的运动，**另外和分割的联合能让深度估计aware物体的尺度，物体的大小通常和深度有直接联系，geonet使用刚性流来替代光流，将相机运动和物体运动隔离开，同时预测物体深度和刚性流**。进一步的发展一定是在线训练，在相机运动的过程中自我训练改进。
2. 立体匹配的话，如何解决低纹理区域处的匹配，如何和语义分割联合，如何提高计算性能。

立体匹配方面的在线训练模型已经出现了，就是madnet，19年的cvpr oral，仔细看了一下基本没用3d conv，所以会不会还有改进的空间也是很有意思的，madnet冻结一部分网络，在线训练只训练底层的几个adaption domain。
![20210824191828](https://raw.githubusercontent.com/lhondong/PicGo/main/img/20210824191828.png)
<center>在线训练的意思是，在运行的时候训练</center>

3. 3d检测点的话，全卷积的架构，如何fuse不同传感器的信息，比如fuse camera和lidar点云，19年出现真正的点云卷积，pointconv和kpconv，相信能为点云分割和3d检测带来更丰富的内容。双目的话，MSRA今年有一篇论文，triangulation learning network，个人感觉很惊艳，使用2d anchor来引导3d anchor。单目6d姿态估计的话，还需要补充。
4. 3d landmark，自监督的方法，如何提高性能，代表性的工作有learable triangulation of human pose，最惊艳的是它的volumetric triangulation，直接将2d heatmap投影到3d heatmap，然后使用3d convnet refine heatmap，个人感觉是一个非常优的架构，但是是否还可以考虑投影part affinity呢，目前part affinity代表一个向量，投影回三维有很严重的不唯一性问题，因为从三维的一个点投影到二维，有很多可能性得到同一个向量，考虑非向量的part affinity是否可以，也是可以思考的。**这里我想到的是直接在二维情况下估计一个3d的paf出来，然后重投影到volume里，也可以估2d的paf，然后重投影的时候认为paf的第三个分量为1，后面再用3d convnet refine。**

![v2-96d31962c712828c0bd0e0f680558315_720w.webp](https://raw.githubusercontent.com/lhondong/PicGo/main/img/v2-96d31962c712828c0bd0e0f680558315_720w.webp.gif)
<center>重投影过程，这样的重投影也许也能用来重投影featuremap，但是volume的大小和分辨率与task直接相关，从而直接影响计算量。一个直接的改进是给多视图每个featuremap一个weightmap，也就是每个点一个权重，加权融合到一起</center>

这是一个非常好的架构，直接把2d提升到了3d，可能被用在多视角的各个领域，包括三维重建，并且最后的结果可以投影回原视角，做自监督，缺点可能是计算量会比较大。

MSRA的一篇论文cross view fusion of human pose也很惊艳，**使用epipolar几何来融合不同视角的2d heatmap达到互相改进的效果，个人感觉这一点不止可以用在landmark上**（凡是使用了heatmap的地方都可以考虑用这种方式fuse，其实不止如此，这个方法会把一个视图里的极限上所有的heatmap值通过一个权重矩阵w加权相加到另一个视图的极线上的点，而这个矩阵本质上是全局的，可能只和对极几何相关，它是否能被用来fuse featuremap个人感觉是非常有意思的一件事，但是这个计算量应该会很大）。

![20210824192146](https://raw.githubusercontent.com/lhondong/PicGo/main/img/20210824192146.png)
<center>fuse可能只和对极几何相关，并能够被用在其他地方，但是计算量会大很多。我跟作者交流过这个方法，可行性是有的，但是问题是参数冗余和计算量大，很明显的其实作者自己也说过，这种连接方式应该沿着极线，而不是所有像素都连接上。</center>

![20210824192206](https://raw.githubusercontent.com/lhondong/PicGo/main/img/20210824192206.png)
<center>fuse只和对极几何有关</center>

这里还推另一篇文章DAVANet: Stereo Deblurring with View Aggregation，这是双目去模糊的，主要思路是使用dispnet提取左右视差，然后将左右featuremap进行warp然后经过fusion layer，这里面有一点问题是，dispnet的监督其实和其他分支是半独立的，fusion layer里面也会考虑把这个dispmap concat起来

![20210824192228](https://raw.githubusercontent.com/lhondong/PicGo/main/img/20210824192228.png)
<center>先估计视差，然后利用视差进行fusion也许才是更合理的做法</center>

dispnet的计算量会比较大，双目特征融合还有一种方法被称为stereo attention，主要思路就是生成两个mask，这个mask表示对应视图极线上的feature对本视图的贡献的权重，

![20210824192255](https://raw.githubusercontent.com/lhondong/PicGo/main/img/20210824192255.png)

关于3d pose，另外有一篇epipolarpose也是自监督的，epipolarpose使用2d pose和对极几何来自监督生成3d posr。

5. 三维重建的话，如何提升重建细节，是否有自监督的算法，代表性的工作有pointmvsnet，rmvsnet。相信meshcnn的出现能被应用到重建里。

![20210824192320](https://raw.githubusercontent.com/lhondong/PicGo/main/img/20210824192320.png)

另外，自监督的mvsnet果然已经出来了。

6. 深度和光流的联合训练，19 年有一篇论文bridge optical flow and depth estimation。3d的flow，sceneflow也就是场景流，这里待补充。
7. 自监督学习很重要，尤其在一些很难获得标注的场景，比如上面说到的立体匹配，深度估计，我在做的时候还遇到过特征点检测和描述子生成，自监督学习通常要有一个定义良好的，即使没有监督数据也能反应问题的loss。弱监督学习也很重要，比如在分割这种标注比较困难的场景。

还有一些传统方法做的比较好的领域也可以尝试，图像自动曝光，图像增强，特征点提取。
细粒度识别，19年的learn to navigate，个人觉得如何构建一个可微分的子模块是一个有意思的问题，难点在nms通过attention module或者learned nms或许有这个希望。centernet出现之后没有nms，或许会改进这个问题。

再补一个方向，scene parsing，如何利用物体之间的先验关系建模提高检测，反过来是否可以帮助无监督或者弱监督学习。

**最后补上我个人的一些想法，深度学习如果高效使用数据，如何做更好的multitasking，一个网络，如果既有检测头，又有分割头，我们希望图像本身既有检测又有分割标注，但是实际上一般是一部分有检测标注，一部分有分割标注，如何发明出一个更好的训练算法充分利用数据是个很大的问题。**我个人探索过交错训练法，也就是以不同的采样率分别训练不同的头，只要数据没有语义冲突，类似的想法应该能work。

**总结一下就是，考虑时间连贯性，考虑多视角，考虑新的传感器和传感器之间的融合，更好的multitasking，更好的训练方法使得数据能被更好的利用，自监督和弱监督的算法，轻量化网络。**