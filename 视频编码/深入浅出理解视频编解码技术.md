# 深入浅出理解视频编解码技术

随着移动互联网技术的蓬勃发展，视频已无处不在。视频直播、视频点播、短视频、视频聊天，已经完全融入了每个人的生活。Cisco 发布的最新报告中写道，到 2022 年，在移动互联网流量中，视频数据占比将高达 82% 。视频为何如此普及呢？是因为通过视频能方便快捷地获取到大量信息。但与此同时，因为视频数据量非常巨大，视频的传输、存储也面临着巨大的挑战。从 20 世纪 90 年代以来，数字视频编解码技术迅速发展，一直是国内外研究的热点领域。视频编解码，将是保证用户高品质视频体验的重要技术。

## 1. 视频编解码技术及标准
视频编解码技术的主要作用，是在可用的计算资源内，追求尽可能高的视频重建质量和尽可能高的压缩比，以达到带宽和存储容量的要求。为何突出“重建质量”？因为视频编码是个有损的过程，用户只能从收到的视频流中解析出“重建”画面，它与原始的画面已经不同，例如观看低质量视频时经常会碰到的“块”效应。如何在一定的带宽占用下，尽可能地保持视频的质量，或者在保持质量情况下，尽可能地减少带宽利用率，是视频编码的基本目标。用专业术语来说，即视频编解码标准的“率失真”性能。“率”是指码率或者带宽占用；“失真”是用来描述重建视频的质量。与编码相对应的是解码或者解压缩过程，是将接收到的或者已经存储在介质上的压缩码流重建成视频信号，然后在各种设备上进行显示。

视频编解码标准，通常只定义上述的解码过程。例如 H.264 / AVC 标准，它定义了什么是符合标准的视频流，对每一个比特的顺序和意义都进行了严格地定义，对如何使用每个比特或者几个比特表达的信息也有精确的定义。正是这样的严格和精确，保证了不同厂商的视频相关服务，可以很方便地兼容在一起，例如用 iPhone、Android Phone 或者 windows PC 都可以观看同一在线视频网站的同一视频。世界上有多个组织进行视频编码标准的制定工作，国际标准组织 ISO 的 MPEG 小组、国际电信联盟 ITU-T 的 VCEG 小组、中国的 AVS 工作组、Google 及各大厂商组成的开放媒体联盟等。

![CMFEG](https://raw.githubusercontent.com/lhondong/PicGo/main/img/CMFEG.png)

自 VCEG 制定 H.120标准开始，视频编码技术不断发展，先后成功地制定了一系列满足不同应用场景的视频编码标准，如图1所示。VCEG 组织先后制定了H.120、H.261、H.262(MPEG-2 Part 2)、H.263、H.263+、H.263++。MPEG也先后制定了MPEG-1、MPEG-2、MPEG-4 Part 2。以及两个国际组织合作制定的H.264/AVC、H.265/HEVC、H.266/VVC；中国自主知识产权的 AVS、AVS2、AVS3 视频编码标准；Google 制定的 VP8、VP9；Google、思科、微软、苹果等公司组成的开放媒体联盟（AOM）制定的 AV1。这里特别要提一下H.264/AVC。H.264/AVC虽有近20年历史，但它优秀的压缩性能、适当的运算复杂度、优秀的开源社区支持、友好的专利政策、强大的生态圈等多个方面的因素，依旧让它保持着强大的生命力，特别是在实时通信领域。像 ZOOM、思科 Webex 等视频会议产品和基于 WebRTC SDK 的视频服务，大多数主流场景都采用 H.264/AVC。

## 2. 混合编码框架
纵观视频标准历史，每一代视频标准都在率失真性能上有着显著的提升，他们都有一个核心的框架，就是基于块的混合编码框架，如图2所示。它是由J. R. Jain 和A. K. Jain在1979年的国际图像编码学会(PCS 1979)上提出了基于块运动补偿和变换编码的混合编码框架。

![量化](https://raw.githubusercontent.com/lhondong/PicGo/main/img/量化.png)

我们一起来对该框架进行拆解和分析。从摄像头采集到的一帧视频，通常是 YUV 格式的原始数据，我们将它划分成多个方形的像素块依次进行处理（例如 H.264/AVC 中以16x16像素为基本单元），进行帧内/帧间预测、正变换、量化、反量化、反变换、环路滤波、熵编码，最后得到视频码流。从视频第一帧的第一个块开始进行空间预测，因当前正在进行编码处理的图像块和其周围的图像块有相似性，我们可以用周围的像素来预测当前的像素。我们将原始像素减去预测像素得到预测残差，再将预测残差进行变换、量化，得到变换系数，然后将其进行熵编码后得到视频码流。

接下来，为了可以使后续的图像块可以使用已经编码过的块进行预测，我们还要对变换系统进行反量化、反变换，得到重建残差，再与预测值进行求合，得到重建图像。最后我们对重建图像进行环路滤波、去除块效应等，这样得到的重建图像，就可以用来对后续图像块进行预测了。按照以上步骤，我们依次对后续图像块进行处理。

对于视频而言，视频帧与帧的间隔大约只有十到几十毫秒，通常拍摄的内容不会发生剧烈变化，它们之间存在非常强的相关性。如图3所示，将视频图像分割成块，在时间相邻的图像之间进行匹配，然后将匹配之后的残差部分进行编码，这样可以较好地去除视频信号中的视频帧与帧之间的冗余，达到视频压缩的目的。这就是运动补偿技术，直到今天它仍然是视频编解码的核心技术之一。

![1623897226603-a8ae9158-8091-4fe9-8d93-87cb5878c685](https://raw.githubusercontent.com/lhondong/PicGo/main/img/1623897226603-a8ae9158-8091-4fe9-8d93-87cb5878c685.webp)

变换编码的核心思想是把视频数据分割成块，利用正交变换将数据的能量集中到较少几个变换系数上。结合量化和熵编码，我们可以获得更有效的压缩。视频编码中信息的损失和压缩比的获得，很大程度上来源于量化模块，就是将源信号中的单一样本映射到某一固定值，形成多到少的映射，从而达到压缩的目的，当然在压缩的过程中就引入了损失。量化后的信号再进行无损的熵编码，消除信号中的统计冗余。熵编码的研究最早可以追溯到 20 世纪 50 年代，经过几十年的发展，熵编码在视频编码中的应用更加成熟、更加精巧，充分利用视频数据中的上下文信息，将概率模型估计得更加准确，从而提高了熵编码的效率。例如H.264/AVC中的Cavlc（基于上下文的变长编码）、Cabac（基于上下文的二进制算术编码）。算术编码技术在后续的视频编码标准，如AV1、HEVC/H.265、VVC/H.266 中也有应用。

视频编码发展至今，VVC/H.266 作为最新制定的标准，采纳了一系列先进的技术，对混合编码框架的各个部分都进行了优化和改进，使得其率失真性能相比前一代标准，又提高了一倍。例如，VVC/H.266 采用了128x128大小的基本编码单元，并且可以继续进行四叉树划分，支持对一个划分进行二分、三分；色度分量独立于亮度分量，支持单独进行划分；更多更精细的帧内预测方向、帧间预测模式；支持多种尺寸和形式的变换、环内滤波等。VVC/H.266 的制定，目标是对多种视频内容有更好支持，例如屏幕共享内容、游戏、动漫、虚拟现实内容（VR、AR）等。其中也有特定的技术被采纳进标准，例如调色板模式、帧内运动补偿、仿射变换、跳过变换、自适应颜色变换等。 

## 3. 提升质量的更多手段
视频编解码标准保证了视频的互通性，视频质量的提升仍然有很多可以深入研究的热点问题。如，基于人眼的主观质量的编码优化、基于AI的编码优化、内容自适应编码等。基于人眼的主观质量优化，主要利用人眼的视觉特性，将掩蔽效应、对比度灵敏度、注意力模型等与编码相结合，合理分配码率、减少编码损失引起的视觉不适。AI在视频编解码领域的应用，包括将多种人工智能算法，如分类器、支持向量机、CNN等对编码参数进行快速选择，也可以使用深度学习对视频进行编码环外与编码环内的处理，如视频超分辨率、去噪、去雾、自适应动态范围调整等编码环外处理，达到提升视频质量的目的；CNN网络取代编码器中的环路滤波、CNN进行帧内预测等编码环内处理，降低编码产生的损失之外，可以进一步提升预测准确度，达到更好的编码效果；此外还有打破传统混合编码框架的深度神经网络编码，如Nvidia的Maxine视频会议服务，利用深度学习来提取特征，然后对特征进行传输以节省带宽。内容自适应技术，也可分成两类，一是编码环内根据编码内容调整编码器的参数，二是编码环外根据要编码的内容进行码率、帧率、分辨率等调整。



# 视频压缩预测技术

视频相关的技术，特别是视频压缩，因其专业性，深入开发的门槛较高。具体到视频实时通信场景，视频压缩技术面临更严峻的挑战，因为实时通信场景下，对时延要求非常高，对设备适配的要求也非常高，对带宽适应的要求也非常高，开发一款满足实时通信要求的编解码器，难度也很高。之前的文章中，我们已经在[《深入浅出理解视频编解码技术》](https://mp.weixin.qq.com/s?__biz=MzU1MjAxNjI0Ng==&mid=2247483931&idx=1&sn=ff0913acd5e2cf4b8cbe219dcd044a73&chksm=fb89c202ccfe4b1467361a3eb0029289d6859ad6a86a2ad49823eadb691c0cda5957a0d2039c&scene=21&token=1347141590&lang=zh_CN#wechat_redirect)一文中简要介绍了视频编解码基本框架，今天我们将深入剖析其中的预测模块，便于大家更好地理解视频编解码技术。

## 1. 颜色空间
开始进入主题之前，先简单看一下视频是如何在计算机中进行表达的。视频是由一系列图片按照时间顺序排列而成，每一张图片为一帧。每一帧可以理解为一个二维矩阵，矩阵的每个元素为一个像素。一个像素通常由三个颜色进行表达，例如用RGB颜色空间表示时，每一个像素由三个颜色分量组成。每一个颜色分量用1个字节来表达，其取值范围就是0~255。编码中常用的YUV格式与之类似，这里不作展开。

![color-intensity](https://raw.githubusercontent.com/lhondong/PicGo/main/img/color-intensity.webp)

以1280x720@60fps的视频序列为例，十秒钟的视频有1280x720x3x60x10 = 1.6GB，如此大量的数据，无论是存储还是传输，都面临巨大的挑战。视频压缩或者编码的目的，也是为了保证视频质量的前提下，将视频减小，以利于传输和存储。同时，为了能正确还原视频，需要将其解码。

从最早的H.261开始，视频编解码的框架都采用了这一结构，如图所示。主要的模块分为帧内/帧间预测、（反）变换、（反）量化、熵编码、环内滤波。一帧视频数据，首先被分割成一系列的方块，按照从左到右从上到下的方式，逐个进行处理，最后得到码流。

![量化](https://raw.githubusercontent.com/lhondong/PicGo/main/img/量化.png)

## 2. 帧内预测
视频数据被划分成方块之后，相邻的方块的像素，以及方块内的像素，颜色往往是逐渐变化的，他们之间有比较强的有相似性。这种相似性，就是空间冗余。既然存在冗余，就可以用更少的数据量来表达这样的特征。比如，先传输第一个像素的值，再传输第二个像素相对于第一个像素的变化值，这个变化值往往取值范围变小了许多，原来要8个bit来表达的像素值，可能只需要少于8个bit就足够了。同样的道理，以像素块为基本单位，也可以进行类似的“差分”操作。我们从示例图中，来更加直观地感受一下这样的相似性。

![179156](https://raw.githubusercontent.com/lhondong/PicGo/main/img/179156.webp)

如图中所标出的两个8x8的块，其亮度分量（Y）沿着“左上到右下”的方向，具有连续性，变化不大。假如我们设计某种特定的“模式”，使其利用左边的块来“预测”右边的块，那么“原始像素”减去“预测像素”就可以减少传输所需要的数据量，同时将该“模式”写入最终的码流，解码器便可以利用左侧的块来“重建”右侧的块。极端一点讲，假如左侧的块的像素值经过一定的运算可以完全和右侧的块相同，那么编码器只要用一个“模式”的代价，传输右侧的块。

当然，视频中的纹理多种多样，单一的模式很难对所有的纹理都适用，因此标准中也设计了多种多样的帧内预测模式，以充分利用像素间的相关性，达到压缩的目的。例如下图所示的H.264中9种帧内预测方向。以模式0（竖直预测）为例，上方块的每个像素值（重建）各复制一列，得到帧内预测值。其它各种模式也采用类似的方法，不过，生成预测值的方式稍有不同。

有这么多的模式，就产生了一个问题，对于一个块而言，我们应该采用哪种模式来进行编码呢？最佳的选择方式，就是遍历所有的模式进行尝试，计算其编码的所需的比特数和产生的质量损失，即率失真优化，这样明显非常复杂，因而也有很多种其它的方式来推断哪种模式更好，例如基于SATD或者边缘检测等。

从H.264的9种预测模式，到AV1的56种帧内方向预测模式，越来越多的模式也是为了更加精准地预测未编码的块，但是模式的增加，一方面增加了传输模式的码率开销，另一方面，从如此重多的模式中选一个最优的模式来编码，使其能达到更高的压缩比，这对编码器的设计和实现也提出了更高的要求。 

